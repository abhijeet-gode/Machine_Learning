{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT in Machine Learning for word embeddings produce by Google for Machine Learning.\n",
    "BERT stands for Bidirectional Encoder Representations from Transformers, are models for pre-trained language representations that can be used to create models for the task of NLP(Natural Language Processing).\n",
    "We can Either use these models to extract high-quality language functionality from text data, or you can refine these models on specific tasks such as classification, feature recognition, answering questions, etc. with data to produce a state of artistic predictions.\n",
    "\n",
    "### Why BERT Embedding for NLP?\n",
    "The BERT embedding are very useful for keyword expansion, semantic search, and other information retrievals, For example, if you want to match customer questions or research to previously answered questions or well-researched research, these representations will help you accurately retrieve result that match customer intent and contextual meaning, even in the absence of overlapping keywords or Phrases.\n",
    "\n",
    "Perhaps the most important reason in that these vectors can be used as high-quality features inputs in the downstream models. NLP models such as LSTM or CNN require inputs in the form of digital vectors, which typically means translating features such as vocabulary and parts of speech into digital representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing BERT Embedding Algorithm\n",
    "Implementing BERT we need to install PyTorch Library. As preferred it strikes a good balance between high-level APIs and TensorFlow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages to get started:\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As BERT is a pre-trained model so the input formatting need to expects input data in a specific format:\n",
    "A Special Token, (SEP) to mark the end of a sentence or the separation between two sentences A special token (CLS), At the start of our text, This token is used for classification task, but BERT expects it regardless of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This is the Sample statement for BERT word Embedding Algorithm [SEP]\n"
     ]
    }
   ],
   "source": [
    "string = 'This is the Sample statement for BERT word Embedding Algorithm'\n",
    "marked_string = '[CLS] ' + string + ' [SEP]'\n",
    "print(marked_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The BERT model provides its Tokenizer, Which we imported above. Let's see how it handles the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'sample', 'statement', 'for', 'bert', 'word', 'em', '##bed', '##ding', 'algorithm', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_string = tokenizer.tokenize(marked_string)\n",
    "print(tokenized_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original text has been split into smaller subsets and characters. The two hash signs that precede some of these subsets are just how our tokenizer indicates that this subsets or character is part of a larger word and is preceded by another subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('this', 2023)\n",
      "('is', 2003)\n",
      "('the', 1996)\n",
      "('sample', 7099)\n",
      "('statement', 4861)\n",
      "('for', 2005)\n",
      "('bert', 14324)\n",
      "('word', 2773)\n",
      "('em', 7861)\n",
      "('##bed', 8270)\n",
      "('##ding', 4667)\n",
      "('algorithm', 9896)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_string)\n",
    "\n",
    "for tup in zip(tokenized_string, indexed_tokens):\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we can prepare word embeddings using the BERT model for any task of NLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3248654ea22e01eba30be24c737f166101737355c2d9a9058a779190f8245d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
